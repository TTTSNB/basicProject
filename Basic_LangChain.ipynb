{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEARNING LANGCHAIN BY USING LANGCHAIN (PYTHON)\n",
    "===============================================\n",
    "\n",
    "BY: ELLIOTT RISCH\n",
    "\n",
    "DATE: 2023-10-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you begin, you will need to create a .env file in the same directory as this notebook. \n",
    "# The .env file should contain the following lines, with your own api keys filled in:\n",
    "\n",
    "OPENAI_API_KEY=\"sk-XXXXXX\"\n",
    "\n",
    "PINECONE_ENV=\"xx-xxxxx-xxx-free\"\n",
    "\n",
    "PINECONE_API_KEY=\"xxxxx-xxxx-xxxx-xxxx-xxxxx\"\n",
    "\n",
    "# You should also have installed the required packages in requirements.txt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I. SETTING UP AND USING --> ChatOpenAI***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Find and load the .env file containing the relevant api keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv # for .env file\n",
    "load_dotenv(find_dotenv()) # load environment variables from .env file\n",
    "\n",
    "# Should read \"True\" below, if the .env file was loaded correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:** Run a quick test query to assure that the OpenAI API key has been loaded correctly from the .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLarge language models use large amounts of data to learn the probability of a word given its context in a given language.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI # Import the model class\n",
    "llm = OpenAI(model_name=\"text-davinci-003\") # Create a new model instance\n",
    "llm(\"explain large language models in one sentence\") # Call the model with a prompt\n",
    "\n",
    "# See the system's response below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:** Call the class imports necessary to talk with a OpenAI chat model, i.e., GPT-3 or GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import( # \"Schema\" here means the mode of communication between the human and the system.\n",
    "    AIMessage,  # What the system says to you. \n",
    "    HumanMessage, # What you use to pose your prompt to the system.\n",
    "    SystemMessage, # What you use to tell the system it's role.\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI # From langchain's chat_models module, we will import the ChatOpenAI class to go with the chosen schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4:** Create a new instance of the OpenAI chat model class, create a list of the components of the chat, and then call the chat function to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.3) # Create a new chat instance\n",
    "messages = [ # Create a list of messages to send to the system\n",
    "        SystemMessage(content=\"You are an expert data scientist.\"), # Tell the system it's role\n",
    "        HumanMessage(content=\"Write a python script that trains a neural network on simulated data.\") # Pose a prompt to the system\n",
    "]\n",
    "response = chat(messages) # Send the messages to the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4:** Display the system's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's an example of a Python script that trains a neural network on simulated data using the Keras library:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "\n",
      "# Generate simulated data\n",
      "np.random.seed(0)\n",
      "X = np.random.rand(1000, 10)\n",
      "y = np.random.randint(2, size=(1000, 1))\n",
      "\n",
      "# Define the model architecture\n",
      "model = Sequential()\n",
      "model.add(Dense(32, input_dim=10, activation='relu'))\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=10, batch_size=32)\n",
      "\n",
      "# Evaluate the model\n",
      "loss, accuracy = model.evaluate(X, y)\n",
      "print(f\"Loss: {loss}\")\n",
      "print(f\"Accuracy: {accuracy}\")\n",
      "```\n",
      "\n",
      "In this script, we first generate simulated data using `numpy`. We create a random matrix `X` of shape `(1000, 10)` and a random binary vector `y` of shape `(1000, 1)`.\n",
      "\n",
      "Then, we define the neural network model using `Sequential` from Keras. The model consists of two dense layers, with 32 units in the first layer and 1 unit in the output layer. The activation function for the first layer is ReLU, and the activation function for the output layer is sigmoid.\n",
      "\n",
      "Next, we compile the model using the binary cross-entropy loss function and the Adam optimizer.\n",
      "\n",
      "We then train the model using the `fit` function, specifying the number of epochs and the batch size.\n",
      "\n",
      "Finally, we evaluate the model on the training data using the `evaluate` function and print the loss and accuracy.\n"
     ]
    }
   ],
   "source": [
    "print(response.content,end='\\n') # Print the system's response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***II. SETTING UP AND USING --> PromptTemplates***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Call the class imports necessary to talk with a OpenAI chat model and to use the PromptTemplates class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI # Import the model class\n",
    "from langchain import PromptTemplate # Import the PromptTemplate class, which allows you to create prompts with variables.\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept {concept} in a couple of sentences.\n",
    "\"\"\" # Create a template for a prompt, with a variable \"concept\" that can be filled in later.\n",
    "\n",
    "prompt = PromptTemplate( # Create a new prompt instance\n",
    "    input_variables=[\"concept\"], # Specify the variables that will be filled in\n",
    "    template=template, # Specify the template to use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:** Print the prompt template for the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expertise in building deep learning models.\\nExplain the concept {concept} in a couple of sentences.\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt # Print the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:** Define the llm and send the prompt template to the chat model with a variable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function that minimizes a cost function. It works by iteratively computing the partial derivatives of the cost function with respect to the parameters and updating the parameters in the negative direction of the gradient of the cost function with respect to the parameters.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\") # Create a new model instance\n",
    "llm(prompt.format(concept=\"gradient descent\")) # Call the model with the prompt, with the variable filled in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***III. SETTING UP AND USING --> Chains***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Make a chain call to the chat model using the prompt template that we have already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient descent is an optimization algorithm used to find the set of parameters that minimize a given cost function. It works by iteratively updating the parameters in the direction of the negative gradient of the cost function with respect to the parameters. The magnitude of the updates is determined by a parameter called the learning rate.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain # Import the LLMChain class, which allows you to chain together multiple language model calls.\n",
    "\n",
    "chain = LLMChain( # Create a new chain instance\n",
    "    llm=llm, # Specify the language model to use\n",
    "    prompt=prompt, # Specify the prompt to use\n",
    ")\n",
    "\n",
    "# run the chain only specifying the input variable.\n",
    "print(chain.run(\"gradient descent\")) # Call the chain with the input variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:** Add a new link to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Turn this({ml_concept}) concept description into an explanation that a five year old child would understand.\" # Specify the template\n",
    "\n",
    "second_prompt = PromptTemplate( # Create a new prompt instance\n",
    "    input_variables=[\"ml_concept\"], # Specify the variables that will be filled in\n",
    "    template=template, # Specify the template to use\n",
    ")   \n",
    "\n",
    "chain_two = LLMChain( # Create a new chain instance \n",
    "    llm=llm, # Specify the language model to use\n",
    "    prompt=second_prompt, # Specify the prompt to use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:** Combine the two links of the chain into a single chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Gradient descent is an iterative optimization algorithm used to find the values of parameters (such as weights and biases) that minimize a given cost function. It works by iteratively making small adjustments to the parameters in the direction of the negative gradient of the cost function with respect to the parameters.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Gradient descent is like a game where you try to find the best way down a mountain. You start at the top and try different paths until you find the one that takes you to the bottom in the least amount of steps.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain # Import the SimpleSequentialChain class, which allows you to chain together multiple language model calls.\n",
    "\n",
    "sequential_chain = SimpleSequentialChain( # Create a new chain instance\n",
    "    chains=[chain, chain_two], # Specify the chains to use\n",
    "    verbose=True, # Specify whether to print the intermediate results\n",
    ")\n",
    "\n",
    "# run the chain only specifying the input variable for the first chain.\n",
    "explanation = sequential_chain.run(\"gradient descent\") # Call the chain with the input variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
